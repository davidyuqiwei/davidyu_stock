19/11/07 11:15:54 WARN Utils: Your hostname, VM_0_11_centos resolves to a loopback address: 127.0.0.1; using 172.16.0.11 instead (on interface eth0)
19/11/07 11:15:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/11/07 11:15:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "INFO".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
19/11/07 11:15:55 INFO SignalUtils: Registered signal handler for INT
19/11/07 11:16:03 INFO SparkContext: Running Spark version 2.4.0
19/11/07 11:16:04 INFO SparkContext: Submitted application: Spark shell
19/11/07 11:16:04 INFO SecurityManager: Changing view acls to: david
19/11/07 11:16:04 INFO SecurityManager: Changing modify acls to: david
19/11/07 11:16:04 INFO SecurityManager: Changing view acls groups to: 
19/11/07 11:16:04 INFO SecurityManager: Changing modify acls groups to: 
19/11/07 11:16:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(david); groups with view permissions: Set(); users  with modify permissions: Set(david); groups with modify permissions: Set()
19/11/07 11:16:04 INFO Utils: Successfully started service 'sparkDriver' on port 35443.
19/11/07 11:16:04 INFO SparkEnv: Registering MapOutputTracker
19/11/07 11:16:04 INFO SparkEnv: Registering BlockManagerMaster
19/11/07 11:16:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/11/07 11:16:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/11/07 11:16:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c1db8a93-c688-42f8-a9b9-c0163ad8240a
19/11/07 11:16:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/11/07 11:16:04 INFO SparkEnv: Registering OutputCommitCoordinator
19/11/07 11:16:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/11/07 11:16:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.0.11:4040
19/11/07 11:16:05 INFO Executor: Starting executor ID driver on host localhost
19/11/07 11:16:05 INFO Executor: Using REPL class URI: spark://172.16.0.11:35443/classes
19/11/07 11:16:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38347.
19/11/07 11:16:05 INFO NettyBlockTransferService: Server created on 172.16.0.11:38347
19/11/07 11:16:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/11/07 11:16:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.0.11, 38347, None)
19/11/07 11:16:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.0.11:38347 with 366.3 MB RAM, BlockManagerId(driver, 172.16.0.11, 38347, None)
19/11/07 11:16:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.0.11, 38347, None)
19/11/07 11:16:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.0.11, 38347, None)
19/11/07 11:16:05 INFO Main: Created Spark session with Hive support
Spark context Web UI available at http://172.16.0.11:4040
Spark context available as 'sc' (master = local[*], app id = local-1573096565306).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.0
      /_/
         
Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_232)
Type in expressions to have them evaluated.
Type :help for more information.

scala> // this script put the   

scala> // target csv file         to the 

scala> // target database.table

scala> /*
     |   ############################
     |   ####### important ##########
     |   ############################
     |   this script first 
     |   @clean the table
     |   @ put all the data into the database
     | */
     | import org.apache.log4j.{Level, Logger}
import org.apache.log4j.{Level, Logger}

scala> object SH_SZ_INDEX{
     |     def main(FileName:String, TableName:String){
     |         // set the LOG level
     |         sc.setLogLevel("ERROR")
     |         val log = Logger.getLogger(this.getClass)
     |         log.warn("this is a warn")
     |         log.warn("this is a warn")
     |           System.out.println(System.getProperty("user.dir"))
     |           var table_name = TableName // set table name 
     |           val work_dir = System.getProperty("user.dir") // the work  dir is the current dir
     |           val data_file = "file://"+work_dir+"/"+FileName // the ful l name of data file we are put into database.table
     |           //
     |         val sqlContext = new org.apache.spark.sql.hive.HiveContext(s c)
     |           val csv_data = spark.read.csv(data_file)
     |           csv_data.show()
     |           csv_data.createOrReplaceTempView("history_day")
     |           // clean the table
     |         val sql_v1 = s"""
     |               truncate table $table_name
     |           """
     |         log.error(sql_v1)
     |         sqlContext.sql(sql_v1)
     |         // put the new file into  database.table
     |           val sql_v2 = s"""
     |               insert into table $table_name
     |               select * from history_day
     |           """
     |           log.error(sql_v2)
     |           //println(sql_v1)
     |           sqlContext.sql(sql_v2)
     |         log.error("+++++++++++++++++++++++++++++++++++++++")
     |         log.error("the program is sucess")
     |         log.error("+++++++++++++++++++++++++++++++++++++++")
     |     }
     | }
warning: there was one deprecation warning; re-run with -deprecation for details
defined object SH_SZ_INDEX

scala> SH_SZ_INDEX.main("all.csv","stock.base_test")
/home/davidyu/stock/scripts/davidyu_stock/scripts/download/basic_info
org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/davidyu/stock/scripts/davidyu_stock/scripts/download/basic_info/all.csv;
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.immutable.List.flatMap(List.scala:355)
  at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)
  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:617)
  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:468)
  at SH_SZ_INDEX$.main(<console>:38)
  ... 49 elided

scala> :quit
